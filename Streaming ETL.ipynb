{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming ETL on Healthcare Payment Data using Structured Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While batched data can provide powerful insights by identifying short- and long-term trends, healthcare providers can combine streaming data with real-time processing to create actionable insights on a minute-by-minute basis.\n",
    "\n",
    "However, building production-grade continuous applications can be challenging, as developers need to overcome many obstacles, including:\n",
    "\n",
    "1. Providing end-to-end reliability\n",
    "\n",
    "The systems must be resilient to failures by ensuring that outputs are consistent with results processed in batch. Additionally, unusual activities (e.g failures in upstream components, traffic spikes, etc.) must be continuously monitored and automatically mitigated to ensure highly available insights are delivered in real-time.\n",
    "\n",
    "2. Performing complex transformations \n",
    "Data arrives in a various formats (CSV, JSON, etc.). We often must restructure, transform the data before it being consumed. Such restructuring requires that all the traditional tools from batch processing systems are available, but without adding latencies.\n",
    "\n",
    "3. Handling late or out of order data\n",
    "Payment data can arrive late or out-of-order. As a result, aggregations and other complex computations must be continuously (and accurately) revised as new information arrives.\n",
    "\n",
    "4. Integrating with other systems\n",
    "Information originates from a variety of sources (Kafka, HDFS, S3, etc), which must be integrated to see the complete picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. where is your input and your final parquet table?\n",
    "\n",
    "### 1. specify the location of the CloudTrail logs files.\n",
    "\n",
    "### 2. specify the output path of parquet table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputPath = \"s3n://MY_INPUTPUT_PATH\"\n",
    "parquetOutputPath = \"/MY_OUTPUT_PATH\"  # DBFS or S3 path "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. the schema of the data?\n",
    "EMR data are JSON files. \n",
    "It is essentially an array (named Payment) of fields related to payment information, some of which are nested structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name ProcessingTime",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c119b1f4ebe1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreaming\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessingTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name ProcessingTime"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.streaming import ProcessingTime\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "ETLPaymentSchema = StructType() \\\n",
    "  .add(\"payment\", ArrayType(StructType() \\\n",
    "    .add(\"date_payment\", StringType()) \\\n",
    "    .add(\"record_id\", StringType()) \\\n",
    "    .add(\"payer\", StringType()) \\\n",
    "    .add(\"amount\", DoubleType()) \\\n",
    "    .add(\"physician_id\", StringType()) \\\n",
    "    .add(\"physician_specialty\", StringType()) \\\n",
    "    .add(\"eventName\", StringType()) \\\n",
    "    .add(\"eventSource\", StringType()) \\\n",
    "    .add(\"eventTime\", StringType()) \\\n",
    "    .add(\"eventType\", StringType()) \\\n",
    "    .add(\"eventVersion\", StringType()) \\\n",
    "    .add(\"readOnly\", BooleanType()) \\\n",
    "    .add(\"recipientAccountId\", StringType()) \\\n",
    "    .add(\"requestID\", StringType()) \\\n",
    "    .add(\"requestParameters\", MapType(StringType(), StringType())) \\\n",
    "    .add(\"resources\", ArrayType(StructType() \\\n",
    "      .add(\"ARN\", StringType()) \\\n",
    "      .add(\"accountId\", StringType()) \\\n",
    "      .add(\"type\", StringType()) \\\n",
    "    )) \\\n",
    "    .add(\"responseElements\", MapType(StringType(), StringType())) \\\n",
    "    .add(\"sharedEventID\", StringType()) \\\n",
    "    .add(\"sourceIPAddress\", StringType()) \\\n",
    "    .add(\"serviceEventDetails\", MapType(StringType(), StringType())) \\\n",
    "    .add(\"userAgent\", StringType()) \\\n",
    "    .add(\"userIdentity\", StructType() \\\n",
    "      .add(\"accessKeyId\", StringType()) \\\n",
    "      .add(\"accountId\", StringType()) \\\n",
    "      .add(\"arn\", StringType()) \\\n",
    "      .add(\"invokedBy\", StringType()) \\\n",
    "      .add(\"principalId\", StringType()) \\\n",
    "      .add(\"sessionContext\", StructType() \\\n",
    "        .add(\"attributes\", StructType() \\\n",
    "          .add(\"creationDate\", StringType()) \\\n",
    "          .add(\"mfaAuthenticated\", StringType()) \\\n",
    "        ) \\\n",
    "        .add(\"sessionIssuer\", StructType() \\\n",
    "          .add(\"accountId\", StringType()) \\\n",
    "          .add(\"arn\", StringType()) \\\n",
    "          .add(\"principalId\", StringType()) \\\n",
    "          .add(\"type\", StringType()) \\\n",
    "          .add(\"userName\", StringType()) \\\n",
    "        )\n",
    "      ) \\\n",
    "      .add(\"type\", StringType()) \\\n",
    "      .add(\"userName\", StringType()) \\\n",
    "      .add(\"webIdFederationData\", StructType() \\\n",
    "        .add(\"federatedProvider\", StringType()) \\\n",
    "        .add(\"attributes\", MapType(StringType(), StringType())) \\\n",
    "      )\n",
    "    ) \\\n",
    "    .add(\"vpcEndpointId\", StringType())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a new structure,\n",
    "which is an array\n",
    "inside of the arry it is a new structure\n",
    "w/ \"additionalEventData\", which is a string type\n",
    "w/......\n",
    "w/ \"resources\", which is an ArrayType( witnew StructType()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. streaming ETL\n",
    "### 1. create the streaming DataFrame that represents the raw records in the files\n",
    "\n",
    "set maxFilesPerTrigger to get earlier access the final Parquet data, as this limit the number of log files processed and written out every trigger.\n",
    "\n",
    "### 2. Transform the data\n",
    "Explode (split) the array of records loaded from each file into separate records.\n",
    "\n",
    "Parse the string event time string in each record to Sparkâ€™s timestamp type.\n",
    "\n",
    "Flatten out the nested columns for easier querying.\n",
    "\n",
    "function: https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/sql/functions.html\n",
    "\n",
    "### 3. Output the data and start stream query\n",
    "\n",
    "Write the data out in the Parquet format,\n",
    "\n",
    "Define the date column from that timestamp and partition the Parquet data by date for efficient time-slice queries.\n",
    "\n",
    "Define the trigger to be every 10 seconds.\n",
    "\n",
    "Define the checkpoint location.\n",
    "\n",
    "Finally, start the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data frame for the raw records\n",
    "rawRecords = spark.readStream \\\n",
    "  .option(\"maxFilesPerTrigger\", \"100\") \\\n",
    "  .schema(ETLPaymentSchema) \\\n",
    "  .json(InputPath)\n",
    "\n",
    "# essientially, the rawRecords is a data frame that new records added \n",
    "# to the stream are like rows being appended to the table\n",
    "# This allows us to treat both batch and streaming data as tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranform the raw data\n",
    "PaymentData = rawRecords \\\n",
    "  .select(explode(\"Records\").alias(\"record\")) \\\n",
    "  .select(\n",
    "    unix_timestamp(\"record.eventTime\", \"yyyy-MM-dd'T'hh:mm:ss\").cast(\"timestamp\").alias(\"timestamp\"),\n",
    "    \"record.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = \"/cloudtrail.checkpoint/\n",
    "# Recovering from Failures to get Exactly-once Fault-tolerance Guarantees\n",
    "\n",
    "\n",
    "streamingETLQuery = cloudTrailEvents \\\n",
    "  .withColumn(\"date\", cloudTrailEvents.timestamp.cast(\"date\")) \\\n",
    "  .writeStream \\\n",
    "  .format(\"parquet\") \\\n",
    "  .option(\"path\", parquetOutputPath) \\\n",
    "  .partitionBy(\"date\") \\\n",
    "  .trigger(processingTime=\"10 seconds\") \\\n",
    "  .option(\"checkpointLocation\", checkpointPath) \\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query up-to-the-minute data from Parquet Table\n",
    "While the streamingETLQuery is continuously converting the data to Parquet, we can already start running ad-hoc queries on the Parquet table. Your queries will always pick up the latest written files while ensuring data consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running example query\n",
    "parquetData = sql(\"select * from parquet.`{}`\".format(parquetOutputPath))\n",
    "display(parquetData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql(\"select * from parquet.`{}`\".format(parquetOutputPath)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured Streaming makes it easy to convert these periodic batch jobs to a real-time data pipeline. \n",
    "\n",
    "1. Filter, transform, and clean up data \n",
    "\n",
    "Raw data is naturally messy and needs to be cleaned up to fit into a well-defined structured format. For example, parsing timestamp strings to date/time types for faster comparisons, filtering corrupted data, nesting/unnesting/flattening complex structures to better organize important columns, etc.\n",
    "\n",
    "2. Convert to a more efficient storage format\n",
    "\n",
    "Text, JSON and CSV data are easy to generate and are human readable, but are very expensive to query. Converting it to more efficient formats like Parquet can reduce file size and improve processing speed.\n",
    "\n",
    "3. Partition data by important columns \n",
    "\n",
    "By partitioning the data based on the value of one or more columns, common queries can be answered more efficiently by reading only the relevant fraction of the total dataset.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
